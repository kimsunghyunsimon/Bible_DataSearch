import streamlit as st
import pandas as pd
import json
from collections import Counter
import re

# ---------------------------------------------------------
# 1. í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ì²˜ë¦¬ ë¡œì§ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# ---------------------------------------------------------

# (1) ë–¼ì–´ë‚¼ ë§ ê¼¬ë¦¬ë“¤ (ê¸¸ì´ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì•¼ ê¸´ ê²ƒë¶€í„° ì˜ë¦½ë‹ˆë‹¤)
# ì—¬ê¸°ì— ê³„ì† ì¶”ê°€í•˜ë©´ 'í•˜ë‚˜ë‹˜ê»˜', 'í•˜ë‚˜ë‹˜ìœ¼ë¡œ' ë“±ì„ ë” ì˜ í•©ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
SUFFIXES = [
    "í•˜ì‚¬", "í•˜ì‹œë‹ˆë¼", "í•˜ì‹œë§¤", "í•˜ë”ë¼", "í•˜ë‹ˆë¼", "í•˜ë¦¬ë¡œë‹¤", # ì„œìˆ ê²© ì–´ë¯¸
    "ê»˜ì„œ", "ì—ê²Œ", "ìœ¼ë¡œ", "ì—ì„œ", "í•˜ê³ ", "ì´ë‚˜", "ê¹Œì§€", "ë¶€í„°", "ì´ë¼", "ë‹ˆë¼", # ê¸´ ì¡°ì‚¬
    "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ê»˜", "ì—¬"  # ì§§ì€ ì¡°ì‚¬
]

# (2) ì œì™¸í•  íŒ¨í„´ (ì•ê¸€ì + ë’·ê¸€ì ì¡°í•©)
# ì˜ˆ: 'ì´'ë¡œ ì‹œì‘í•˜ê³  'ê²ƒ'ìœ¼ë¡œ ëë‚˜ëŠ” 2~3ê¸€ì -> ì œê±°
IGNORE_STARTS = {'ì´', 'ê·¸', 'ì €', 'ë‚´', 'ë„¤', 'ë‚˜', 'ë„ˆ', 'ìš°', 'ì', 'ëˆ„'}
IGNORE_ENDS = {'ê²ƒ', 'ë“¤', 'ë“±', 'ì¤‘', 'ë¿', 'ì¯¤', 'ìœ„', 'ê°€', 'ëŠ”', 'ë„', 'ë¥¼', 'ì€'}

def normalize_word(word):
    """
    ë‹¨ì–´ì˜ ê¼¬ë¦¬(ì¡°ì‚¬)ë¥¼ ìë¥´ê³  ê¸°ë³¸í˜•ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    ì˜ˆ: ì—¬í˜¸ì™€ê»˜ì„œ -> ì—¬í˜¸ì™€, í•˜ë‚˜ë‹˜ì´ -> í•˜ë‚˜ë‹˜
    """
    original_word = word
    # ê¸¸ì´ê°€ 2ê¸€ì ì´ìƒì¼ ë•Œë§Œ ì¡°ì‚¬ë¥¼ ë–¼ì–´ëƒ…ë‹ˆë‹¤ (í•œ ê¸€ì ë‹¨ì–´ ë³´í˜¸)
    if len(word) < 2:
        return word
        
    for suffix in SUFFIXES:
        if word.endswith(suffix):
            # ì¡°ì‚¬ë¥¼ ë—ì„ ë•Œ ë„ˆë¬´ ì§§ì•„ì§€ë©´(1ê¸€ì) ì›ë˜ëŒ€ë¡œ ë‘˜ì§€, ë—„ì§€ ê²°ì •
            # ì—¬ê¸°ì„œëŠ” ì¡°ì‚¬ë¥¼ ë–¼ì–´ëƒ…ë‹ˆë‹¤. (ì˜ˆ: 'ì™•ì´' -> 'ì™•')
            stem = word[:-len(suffix)]
            if len(stem) >= 1: 
                return stem
    return word

def is_stop_pattern(word):
    """
    ì‚¬ìš©ì ìš”ì²­ íŒ¨í„´ í•„í„°ë§:
    (ì´, ê·¸, ì €, ë‚´...) + (ëŠ”, ê°€, ê²ƒ, ë“¤...) í˜•íƒœì˜ 2~3ìŒì ˆ ë‹¨ì–´ ì œì™¸
    """
    # 1. ê¸¸ì´ ì²´í¬ (2~3ê¸€ì)
    if len(word) in [2, 3]:
        # 2. ì•ê¸€ì ì²´í¬
        if word[0] in IGNORE_STARTS:
            # 3. ë’·ê¸€ì ì²´í¬ (í˜¹ì€ ê¼¬ë¦¬ë¥¼ ë—€ ìƒíƒœì—ì„œë„ ì²´í¬)
            if word[-1] in IGNORE_ENDS:
                return True
            # ì˜ˆ: 'ê·¸ê°€', 'ì´ê²ƒ', 'ì €í¬' ë“±
            
    # ì¶”ê°€ë¡œ ì œì™¸í•˜ê³  ì‹¶ì€ íŠ¹ì • ë‹¨ì–´ë“¤
    if word in ["ê°€ë¼ì‚¬ëŒ€", "ì´ë¥´ì‹œë˜", "ëŒ€ë‹µí•˜ì—¬", "ìˆëŠë‹ˆë¼", "í•˜ì˜€ë”ë¼"]:
        return True
        
    return False

# ---------------------------------------------------------
# 2. ì„±ê²½ ë©”íƒ€ë°ì´í„°
# ---------------------------------------------------------
OT_BOOKS = [
    "ì°½ì„¸ê¸°", "ì¶œì• êµ½ê¸°", "ë ˆìœ„ê¸°", "ë¯¼ìˆ˜ê¸°", "ì‹ ëª…ê¸°", "ì—¬í˜¸ìˆ˜ì•„", "ì‚¬ì‚¬ê¸°", "ë£»ê¸°",
    "ì‚¬ë¬´ì—˜ìƒ", "ì‚¬ë¬´ì—˜í•˜", "ì—´ì™•ê¸°ìƒ", "ì—´ì™•ê¸°í•˜", "ì—­ëŒ€ìƒ", "ì—­ëŒ€í•˜", "ì—ìŠ¤ë¼", "ëŠí—¤ë¯¸ì•¼",
    "ì—ìŠ¤ë”", "ìš¥ê¸°", "ì‹œí¸", "ì ì–¸", "ì „ë„ì„œ", "ì•„ê°€", "ì´ì‚¬ì•¼", "ì˜ˆë ˆë¯¸ì•¼",
    "ì˜ˆë ˆë¯¸ì•¼ì• ê°€", "ì—ìŠ¤ê²”", "ë‹¤ë‹ˆì—˜", "í˜¸ì„¸ì•„", "ìš”ì—˜", "ì•„ëª¨ìŠ¤", "ì˜¤ë°”ëŒœ", "ìš”ë‚˜",
    "ë¯¸ê°€", "ë‚˜í›”", "í•˜ë°•êµ­", "ìŠ¤ë°”ëƒ", "í•™ê°œ", "ìŠ¤ê°€ë´", "ë§ë¼ê¸°"
]
NT_BOOKS = [
    "ë§ˆíƒœë³µìŒ", "ë§ˆê°€ë³µìŒ", "ëˆ„ê°€ë³µìŒ", "ìš”í•œë³µìŒ", "ì‚¬ë„í–‰ì „", "ë¡œë§ˆì„œ", "ê³ ë¦°ë„ì „ì„œ", "ê³ ë¦°ë„í›„ì„œ",
    "ê°ˆë¼ë””ì•„ì„œ", "ì—ë² ì†Œì„œ", "ë¹Œë¦½ë³´ì„œ", "ê³¨ë¡œìƒˆì„œ", "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ", "ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ", "ë””ëª¨ë°ì „ì„œ", "ë””ëª¨ë°í›„ì„œ",
    "ë””ë„ì„œ", "ë¹Œë ˆëª¬ì„œ", "íˆë¸Œë¦¬ì„œ", "ì•¼ê³ ë³´ì„œ", "ë² ë“œë¡œì „ì„œ", "ë² ë“œë¡œí›„ì„œ", "ìš”í•œì¼ì„œ", "ìš”í•œì´ì„œ",
    "ìš”í•œì‚¼ì„œ", "ìœ ë‹¤ì„œ", "ìš”í•œê³„ì‹œë¡"
]
ALL_BOOKS_ORDER = OT_BOOKS + NT_BOOKS

def get_testament(book_name):
    if book_name in OT_BOOKS: return "êµ¬ì•½"
    elif book_name in NT_BOOKS: return "ì‹ ì•½"
    return "ê¸°íƒ€"

# ---------------------------------------------------------
# 3. ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------
@st.cache_data
def load_data(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        return pd.DataFrame()

    rows = []
    for book, chapters in data.items():
        for chapter, verses in chapters.items():
            for verse, content in verses.items():
                text = content.get("text", "")
                rows.append({
                    "book": book,
                    "chapter": int(chapter),
                    "verse": int(verse),
                    "text": text,
                    "testament": get_testament(book)
                })
    if not rows: return pd.DataFrame()
    df = pd.DataFrame(rows)
    df['book'] = pd.Categorical(df['book'], categories=ALL_BOOKS_ORDER, ordered=True)
    df = df.sort_values(by=['book', 'chapter', 'verse']).reset_index(drop=True)
    return df

# ---------------------------------------------------------
# 4. ë¶„ì„ í•¨ìˆ˜ (ë¡œì§ ì ìš©ë¨)
# ---------------------------------------------------------
def get_top_words(df, n=10):
    full_text = " ".join(df['text'].tolist())
    words = re.findall(r'\w+', full_text)
    
    processed_words = []
    for w in words:
        # 1. ê¼¬ë¦¬ ìë¥´ê¸° (ì—¬í˜¸ì™€ê»˜ì„œ -> ì—¬í˜¸ì™€)
        stem = normalize_word(w)
        
        # 2. íŒ¨í„´ í•„í„°ë§ (ì´ê²ƒ, ê·¸ê°€ -> ì œì™¸)
        if not is_stop_pattern(w) and not is_stop_pattern(stem):
            # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if len(stem) > 1: # í•œ ê¸€ì ë‹¨ì–´ë„ ëº„ê¹Œìš”? (í•„ìš”ì‹œ ì‚­ì œ ê°€ëŠ¥)
                processed_words.append(stem)
    
    return Counter(processed_words).most_common(n)

def search_word_in_bible(df, keyword):
    count = 0
    results = []
    keyword = keyword.strip()
    if not keyword: return 0, []

    for _, row in df.iterrows():
        text = row['text']
        c = text.count(keyword)
        if c > 0:
            count += c
            results.append(f"[{row['book']} {row['chapter']}:{row['verse']}] {text}")
    return count, results

# ---------------------------------------------------------
# 5. UI êµ¬ì„±
# ---------------------------------------------------------
st.set_page_config(page_title="ì„±ê²½ ë°ì´í„° ë¶„ì„", layout="wide")
st.title("ğŸ“– ì„±ê²½ ë¹…ë°ì´í„° ë¶„ì„ê¸°")

df = load_data("bible_data.json")

if not df.empty:
    st.sidebar.header("ğŸ” ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •")
    scope = st.sidebar.radio("ë²”ìœ„ ì„ íƒ", ["ì„±ê²½ ì „ì²´", "êµ¬ì•½ë§Œ", "ì‹ ì•½ë§Œ", "ì±… ë³„ë¡œ ì„ íƒ"])

    target_df = df.copy()
    if scope == "êµ¬ì•½ë§Œ": target_df = df[df['testament'] == "êµ¬ì•½"]
    elif scope == "ì‹ ì•½ë§Œ": target_df = df[df['testament'] == "ì‹ ì•½"]
    elif scope == "ì±… ë³„ë¡œ ì„ íƒ":
        available_books = [b for b in ALL_BOOKS_ORDER if b in df['book'].unique()]
        sel = st.sidebar.selectbox("ì„±ê²½ì±… ì„ íƒ", available_books)
        target_df = df[df['book'] == sel]

    st.info(f"ë¶„ì„ ëŒ€ìƒ: **{scope}** ({len(target_df):,} êµ¬ì ˆ)")

    tab1, tab2 = st.tabs(["ğŸ“Š ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ (Top 10)", "ğŸ” ë‹¨ì–´ ì°¾ê¸°"])

    with tab1:
        st.subheader("ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ Top 10")
        st.caption("â€» 'ì—¬í˜¸ì™€ê»˜ì„œ'ëŠ” 'ì—¬í˜¸ì™€'ë¡œ í•©ì¹˜ê³ , 'ì´ê²ƒ/ì €ê²ƒ' ë“±ì€ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")
        
        if st.button("ë¶„ì„ ì‹œì‘", key="btn_top"):
            with st.spinner("ë‹¨ì–´ ì •ì œ ë° ë¶„ì„ ì¤‘..."):
                top_list = get_top_words(target_df, 10)
                top_df = pd.DataFrame(top_list, columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
                col1, col2 = st.columns([1, 2])
                with col1: st.table(top_df)
                with col2: st.bar_chart(top_df.set_index("ë‹¨ì–´"))

    with tab2:
        st.subheader("ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê²€ìƒ‰")
        kwd = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")
        if kwd:
            cnt, vss = search_word_in_bible(target_df, kwd)
            st.success(f"'{kwd}' í¬í•¨ ì´ **{cnt}ë²ˆ** ë“±ì¥")
            if vss:
                with st.expander("êµ¬ì ˆ ë³´ê¸°"):
                    for v in vss: st.text(v)
