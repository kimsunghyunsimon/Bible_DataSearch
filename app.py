import streamlit as st
import pandas as pd
import json
from collections import Counter
import re

# ---------------------------------------------------------
# 1. ì„¤ì •: í†µí•© ê·œì¹™ ë° ë¶ˆìš©ì–´
# ---------------------------------------------------------

# (1) ì´ ë‹¨ì–´ë“¤ì€ ì´ê±¸ë¡œ í•©ì¹œë‹¤ (Merge Rules)
MERGE_RULES = {
    "ì´ë¥´ì‹œë˜": "ì´ë¥´ë˜",
    "ê°€ë¼ì‚¬ëŒ€": "ì´ë¥´ë˜",  # (ì„ íƒì‚¬í•­) ê°€ë¼ì‚¬ëŒ€ë„ ì´ë¥´ë˜ë¡œ í•©ì¹˜ê³  ì‹¶ìœ¼ì‹œë©´ ìœ ì§€, ì•„ë‹ˆë©´ ì´ ì¤„ ì‚­ì œ
}

# (2) ë¬´ì¡°ê±´ ì œì™¸í•  ë‹¨ì–´ (Exact Match)
STOPWORDS_EXACT = {
    "ìœ„í•˜", "ê²ƒì´", "ë„ˆí¬", "ë„ˆí¬ê°€", "ë„ˆí¬ëŠ”", "ë‚´ê°€", "ë„¤ê°€",
    "ê·¸", "ì´", "ì €", "ë‚´", "ë„¤", "ë‚˜", "ë„ˆ", "ìš°ë¦¬",
    "ìˆë‹¤", "ìˆëŠ”", "ìˆì–´", "í•˜ë‹ˆ", "í•˜ë‚˜", "í•˜ë¼"
}

# (3) ë–¼ì–´ë‚¼ ì¡°ì‚¬/ì–´ë¯¸ (ê¸´ ê²ƒë¶€í„°)
SUFFIXES = [
    "í•˜ì‚¬", "í•˜ì‹œë‹ˆë¼", "í•˜ì‹œë§¤", "í•˜ë”ë¼", "í•˜ë‹ˆë¼", "í•˜ë¦¬ë¡œë‹¤", 
    "ê»˜ì„œ", "ì—ê²Œ", "ìœ¼ë¡œ", "ì—ì„œ", "í•˜ê³ ", "ì´ë‚˜", "ê¹Œì§€", "ë¶€í„°", "ì´ë¼", "ë‹ˆë¼",
    "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ê»˜", "ì—¬"
]

# (4) íŒ¨í„´ìœ¼ë¡œ ì œì™¸ (ì‹œì‘+ë ê¸€ì ì¡°í•©)
IGNORE_STARTS = {'ì´', 'ê·¸', 'ì €', 'ë‚´', 'ë„¤', 'ë‚˜', 'ë„ˆ', 'ìš°', 'ì', 'ëˆ„'}
IGNORE_ENDS = {'ê²ƒ', 'ë“¤', 'ë“±', 'ì¤‘', 'ë¿', 'ì¯¤', 'ìœ„', 'ê°€', 'ëŠ”', 'ë„', 'ë¥¼', 'ì€'}

def normalize_word(word):
    """ì¡°ì‚¬ ìë¥´ê¸°"""
    if len(word) < 2: return word
    for suffix in SUFFIXES:
        if word.endswith(suffix):
            stem = word[:-len(suffix)]
            if len(stem) >= 1: return stem
    return word

def is_stop_pattern(word):
    """ë¶ˆìš©ì–´ íŒ¨í„´ í•„í„°ë§"""
    if len(word) not in [2, 3]: return False
    
    # 1. ë‹¨ì–´ ìì²´ì— í¬í•¨ë˜ë©´ ì•ˆ ë˜ëŠ” ê²ƒë“¤
    if "ë„ˆí¬" in word or "ìœ„í•˜" in word: return True

    # 2. ì‹œì‘+ë íŒ¨í„´
    if word[0] in IGNORE_STARTS and word[-1] in IGNORE_ENDS: return True
    
    return False

# ---------------------------------------------------------
# 2. ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------
OT_BOOKS = [
    "ì°½ì„¸ê¸°", "ì¶œì• êµ½ê¸°", "ë ˆìœ„ê¸°", "ë¯¼ìˆ˜ê¸°", "ì‹ ëª…ê¸°", "ì—¬í˜¸ìˆ˜ì•„", "ì‚¬ì‚¬ê¸°", "ë£»ê¸°",
    "ì‚¬ë¬´ì—˜ìƒ", "ì‚¬ë¬´ì—˜í•˜", "ì—´ì™•ê¸°ìƒ", "ì—´ì™•ê¸°í•˜", "ì—­ëŒ€ìƒ", "ì—­ëŒ€í•˜", "ì—ìŠ¤ë¼", "ëŠí—¤ë¯¸ì•¼",
    "ì—ìŠ¤ë”", "ìš¥ê¸°", "ì‹œí¸", "ì ì–¸", "ì „ë„ì„œ", "ì•„ê°€", "ì´ì‚¬ì•¼", "ì˜ˆë ˆë¯¸ì•¼",
    "ì˜ˆë ˆë¯¸ì•¼ì• ê°€", "ì—ìŠ¤ê²”", "ë‹¤ë‹ˆì—˜", "í˜¸ì„¸ì•„", "ìš”ì—˜", "ì•„ëª¨ìŠ¤", "ì˜¤ë°”ëŒœ", "ìš”ë‚˜",
    "ë¯¸ê°€", "ë‚˜í›”", "í•˜ë°•êµ­", "ìŠ¤ë°”ëƒ", "í•™ê°œ", "ìŠ¤ê°€ë´", "ë§ë¼ê¸°"
]
NT_BOOKS = [
    "ë§ˆíƒœë³µìŒ", "ë§ˆê°€ë³µìŒ", "ëˆ„ê°€ë³µìŒ", "ìš”í•œë³µìŒ", "ì‚¬ë„í–‰ì „", "ë¡œë§ˆì„œ", "ê³ ë¦°ë„ì „ì„œ", "ê³ ë¦°ë„í›„ì„œ",
    "ê°ˆë¼ë””ì•„ì„œ", "ì—ë² ì†Œì„œ", "ë¹Œë¦½ë³´ì„œ", "ê³¨ë¡œìƒˆì„œ", "ë°ì‚´ë¡œë‹ˆê°€ì „ì„œ", "ë°ì‚´ë¡œë‹ˆê°€í›„ì„œ", "ë””ëª¨ë°ì „ì„œ", "ë””ëª¨ë°í›„ì„œ",
    "ë””ë„ì„œ", "ë¹Œë ˆëª¬ì„œ", "íˆë¸Œë¦¬ì„œ", "ì•¼ê³ ë³´ì„œ", "ë² ë“œë¡œì „ì„œ", "ë² ë“œë¡œí›„ì„œ", "ìš”í•œì¼ì„œ", "ìš”í•œì´ì„œ",
    "ìš”í•œì‚¼ì„œ", "ìœ ë‹¤ì„œ", "ìš”í•œê³„ì‹œë¡"
]
ALL_BOOKS_ORDER = OT_BOOKS + NT_BOOKS

@st.cache_data
def load_data(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError: return pd.DataFrame()

    rows = []
    for book, chapters in data.items():
        for chapter, verses in chapters.items():
            for verse, content in verses.items():
                rows.append({
                    "book": book,
                    "chapter": int(chapter),
                    "verse": int(verse),
                    "text": content.get("text", ""),
                    "testament": "êµ¬ì•½" if book in OT_BOOKS else ("ì‹ ì•½" if book in NT_BOOKS else "ê¸°íƒ€")
                })
    if not rows: return pd.DataFrame()
    df = pd.DataFrame(rows)
    df['book'] = pd.Categorical(df['book'], categories=ALL_BOOKS_ORDER, ordered=True)
    df = df.sort_values(by=['book', 'chapter', 'verse']).reset_index(drop=True)
    return df

# ---------------------------------------------------------
# 3. í•µì‹¬ ë¶„ì„ í•¨ìˆ˜ (ëŒ€í­ ìµœì í™”ë¨)
# ---------------------------------------------------------
def get_top_words_fast(df, n=10):
    """
    ì†ë„ ê°œì„  ë²„ì „: 
    1. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ë¨¼ì € ì¶”ì¶œ (Cì–¸ì–´ ê¸°ë°˜ re ëª¨ë“ˆ ì‚¬ìš© -> ë¹ ë¦„)
    2. ì¤‘ë³µë˜ëŠ” ë‹¨ì–´ë“¤(vocabulary)ì— ëŒ€í•´ì„œë§Œ ì •ì œ ë¡œì§ ìˆ˜í–‰ (ë°˜ë³µ íšŸìˆ˜ ê¸‰ê°)
    """
    # 1. ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ì„±ê²½ ì „ì²´ í…ìŠ¤íŠ¸ëŠ” ëª‡ MB ì•ˆë˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë¬¸ì œ ì—†ìŒ)
    full_text = " ".join(df['text'].tolist())
    
    # 2. ëª¨ë“  ë‹¨ì–´ ì¶”ì¶œ (raw tokens)
    raw_words = re.findall(r'\w+', full_text)
    
    # 3. ì¼ë‹¨ ì„¼ë‹¤ (Raw Count)
    raw_counter = Counter(raw_words)
    
    # 4. ì¤‘ë³µ ì—†ì´ ìœ ë‹ˆí¬í•œ ë‹¨ì–´ë“¤ë§Œ êº¼ë‚´ì„œ ì •ì œ ë¡œì§ ìˆ˜í–‰
    final_counter = Counter()
    
    for word, count in raw_counter.items():
        # (1) í†µí•© ê·œì¹™ ì ìš© (ì´ë¥´ì‹œë˜ -> ì´ë¥´ë˜)
        if word in MERGE_RULES:
            target_word = MERGE_RULES[word]
            final_counter[target_word] += count
            continue
            
        # (2) ì¡°ì‚¬ ìë¥´ê¸°
        stem = normalize_word(word)
        
        # (3) ë¶ˆìš©ì–´/íŒ¨í„´ í•„í„°ë§
        # ì›ë³¸ ë‹¨ì–´(word)ë‚˜ ì •ì œëœ ë‹¨ì–´(stem)ê°€ ë¶ˆìš©ì–´ë©´ íŒ¨ìŠ¤
        if stem in STOPWORDS_EXACT or is_stop_pattern(stem) or is_stop_pattern(word):
            continue
            
        if len(stem) > 1:
            final_counter[stem] += count
            
    return final_counter.most_common(n)

def search_word_in_bible(df, keyword):
    count = 0
    results = []
    keyword = keyword.strip()
    if not keyword: return 0, []

    # ê²€ìƒ‰ì€ ë‹¨ìˆœ í¬í•¨ ì—¬ë¶€ì´ë¯€ë¡œ ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (ì†ë„ ì¶©ë¶„)
    # ë‹¤ë§Œ ë„ˆë¬´ ëŠë¦¬ë©´ str.containsë¡œ ë²¡í„°í™” ê°€ëŠ¥í•˜ì§€ë§Œ, 
    # ìƒì„¸ êµ¬ì ˆ ì¶”ì¶œì„ ìœ„í•´ loop ìœ ì§€ (í˜„ëŒ€ ì»´í“¨í„°ì—ì„œ ì¶©ë¶„íˆ ë¹ ë¦„)
    for _, row in df.iterrows():
        text = row['text']
        c = text.count(keyword)
        if c > 0:
            count += c
            results.append(f"[{row['book']} {row['chapter']}:{row['verse']}] {text}")
    return count, results

# ---------------------------------------------------------
# 4. UI êµ¬ì„±
# ---------------------------------------------------------
st.set_page_config(page_title="ì„±ê²½ ë°ì´í„° ë¶„ì„", layout="wide")
st.title("ğŸ“– ì„±ê²½ ë¹…ë°ì´í„° ë¶„ì„ê¸°")

df = load_data("bible_data.json")

if not df.empty:
    st.sidebar.header("ğŸ” ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •")
    scope = st.sidebar.radio("ë²”ìœ„ ì„ íƒ", ["ì„±ê²½ ì „ì²´", "êµ¬ì•½ë§Œ", "ì‹ ì•½ë§Œ", "ì±… ë³„ë¡œ ì„ íƒ"])

    target_df = df.copy()
    if scope == "êµ¬ì•½ë§Œ": target_df = df[df['testament'] == "êµ¬ì•½"]
    elif scope == "ì‹ ì•½ë§Œ": target_df = df[df['testament'] == "ì‹ ì•½"]
    elif scope == "ì±… ë³„ë¡œ ì„ íƒ":
        available_books = [b for b in ALL_BOOKS_ORDER if b in df['book'].unique()]
        sel = st.sidebar.selectbox("ì„±ê²½ì±… ì„ íƒ", available_books)
        target_df = df[df['book'] == sel]

    st.info(f"ë¶„ì„ ëŒ€ìƒ: **{scope}** ({len(target_df):,} êµ¬ì ˆ)")

    tab1, tab2 = st.tabs(["ğŸ“Š ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ (Top 10)", "ğŸ” ë‹¨ì–´ ì°¾ê¸°"])

    with tab1:
        st.subheader("ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ Top 10")
        st.caption("â€» 'ì´ë¥´ì‹œë˜'ëŠ” 'ì´ë¥´ë˜'ë¡œ í•©ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if st.button("ë¶„ì„ ì‹œì‘", key="btn_top"):
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°”ëŠ” ì—†ì•´ìŠµë‹ˆë‹¤. (ìˆœì‹ê°„ì— ëë‚˜ë¯€ë¡œ)
            top_list = get_top_words_fast(target_df, 10)
            
            top_df = pd.DataFrame(top_list, columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
            top_df.index = top_df.index + 1
            
            col1, col2 = st.columns([1, 2])
            with col1: st.table(top_df)
            with col2: st.bar_chart(top_df.set_index("ë‹¨ì–´"))

    with tab2:
        st.subheader("ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê²€ìƒ‰")
        kwd = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")
        if kwd:
            cnt, vss = search_word_in_bible(target_df, kwd)
            st.success(f"'{kwd}' í¬í•¨ ì´ **{cnt}ë²ˆ** ë“±ì¥")
            if vss:
                with st.expander("êµ¬ì ˆ ë³´ê¸°"):
                    for v in vss: st.text(v)
